{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def c_import(library, elements=None, name=None, always_reimport= True, always_reinstall = True):\n",
    "  if elements:\n",
    "    import_str = f'from {library} import {\", \".join(elements)}'\n",
    "    tested_install_var = \", \".join(elements)\n",
    "    any_not_installed = True if any(e not in globals() for e in elements) else False\n",
    "  else:\n",
    "    import_str = f'import {library}'\n",
    "    tested_install_var = library\n",
    "    any_not_installed = True if library not in globals() else False\n",
    "  if name:\n",
    "    import_str = f'{import_str} as {name}'\n",
    "    tested_install_var = name\n",
    "    any_not_installed = True if name not in globals() else False\n",
    "\n",
    "  def sub_install():\n",
    "    subprocess.run(f'pip install {library}', shell=True, check=True)\n",
    "    print(f'Library {library} installed successfully.')\n",
    "\n",
    "  def sub_import():\n",
    "    exec(import_str, globals())\n",
    "    print(f'Library {library} imported successfully. As: \\n {import_str}')\n",
    "\n",
    "  if always_reinstall:\n",
    "    try:\n",
    "      sub_install()\n",
    "      sub_import()\n",
    "    except subprocess.CalledProcessError:\n",
    "      print(f'Failed to install {library}.')\n",
    "    except ImportError as err:\n",
    "      print(f'After Install. Import error: {err}')\n",
    "\n",
    "  else:\n",
    "    if always_reimport == True or any_not_installed == True:\n",
    "        try:\n",
    "          sub_import()\n",
    "        except ImportError as err:\n",
    "          print(f'Import error: {err}')\n",
    "          if library in str(err):\n",
    "            try:\n",
    "                # Use subprocess to run the pip install command\n",
    "                sub_install()\n",
    "                sub_import()\n",
    "            except subprocess.CalledProcessError:\n",
    "                print(f'Failed to install {library}.')\n",
    "    else:\n",
    "      print(f'\"{tested_install_var}\" already installed and imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library pandas imported successfully. As: \n",
      " import pandas as pd\n",
      "Library csv imported successfully. As: \n",
      " import csv\n",
      "Library json imported successfully. As: \n",
      " import json\n",
      "Library os imported successfully. As: \n",
      " import os\n",
      "\"subprocess\" already installed and imported\n",
      "Library tqdm.notebook imported successfully. As: \n",
      " from tqdm.notebook import tqdm\n",
      "Library concurrent.futures imported successfully. As: \n",
      " from concurrent.futures import ProcessPoolExecutor\n",
      "Library datetime imported successfully. As: \n",
      " import datetime\n",
      "Library argparse imported successfully. As: \n",
      " import argparse\n",
      "Library codecs imported successfully. As: \n",
      " import codecs\n",
      "\"os\" already installed and imported\n",
      "Library sys imported successfully. As: \n",
      " import sys\n",
      "Library numpy imported successfully. As: \n",
      " import numpy as np\n"
     ]
    }
   ],
   "source": [
    "import_config = {'always_reimport': False, 'always_reinstall': False}\n",
    "c_import('pandas',name='pd',**import_config)\n",
    "c_import('csv',**import_config)\n",
    "c_import('json',**import_config)\n",
    "c_import('os',**import_config)\n",
    "c_import('subprocess',**import_config)\n",
    "c_import('tqdm.notebook',['tqdm'],**import_config)\n",
    "c_import('concurrent.futures',['ProcessPoolExecutor'],**import_config)\n",
    "c_import('datetime',**import_config)\n",
    "c_import('argparse', **import_config)\n",
    "c_import('codecs', **import_config)\n",
    "c_import('os', **import_config)\n",
    "c_import('sys', **import_config)\n",
    "c_import('numpy',name='np', **import_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalids(col='', df=''):\n",
    "  invalid_options = [np.nan, 'nan', None, 0, '0', 'NaN', '[deleted]', '[removed]']\n",
    "  df = df.dropna(subset=[col], how='all')\n",
    "  df = df[~df[col].isin(invalid_options)]\n",
    "  return df\n",
    "\n",
    "# for source in reddit_files_titles:\n",
    "source = 'argentina_comments'\n",
    "source_dir = f'../output/reddit_output/{source}-output_table.csv'\n",
    "source_output_dir = f'../output/reddit_output/filtered_tables_LIWC_count/{source}-liwc_output.csv'\n",
    "\n",
    "df_output_table = pd.read_csv(source_dir,sep=',')\n",
    "\n",
    "df_output_table = remove_invalids('text', df_output_table)\n",
    "\n",
    "if 'title' in df_output_table.columns: #if its a submission (has title)\n",
    "    #remove invalid/incomplete obs\n",
    "    df_output_table = remove_invalids('title', df_output_table)\n",
    "    #join title and text (yes fillna just in case something passed previous cleanup)\n",
    "    df_output_table['text'] = df_output_table['title'].fillna('') + '\\n' + df_output_table['text'].fillna('')\n",
    "    #now drop it\n",
    "    df_output_table = df_output_table.drop(columns=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "from dask.multiprocessing import get\n",
    "\n",
    "# Read the large CSV using Dask\n",
    "ddf = dd.read_csv('/Volumes/Drak√¥n Kholkikos - 2TB/Tesis-Grado/programs/liwc-tmp.csv')\n",
    "\n",
    "# Process the 'text' column using Dask's bag\n",
    "liwc_counts = ddf['text'].to_bag().map(liwc().getLIWCCount)\n",
    "\n",
    "# Save intermediate results to a Dask DataFrame\n",
    "ddf['liwc_counts'] = liwc_counts\n",
    "\n",
    "# Persist the Dask DataFrame to optimize performance\n",
    "ddf = ddf.persist()\n",
    "\n",
    "# Compute the Dask DataFrame to perform the processing\n",
    "ddf = ddf.compute(get=get)\n",
    "\n",
    "# Continue with the rest of your code to concatenate the results\n",
    "liwc_df = pd.DataFrame(ddf['liwc_counts'].to_list())\n",
    "\n",
    "# Add 'text_' as a prefix to all column names\n",
    "liwc_df = liwc_df.add_prefix('text_')\n",
    "\n",
    "# Concatenate the new DataFrame with the original DataFrame\n",
    "df_output_table = pd.concat([df_output_table, liwc_df], axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv-tesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
